"""
Unit tests for QA Agent.

Tests the AI agent specialized in testing, quality assurance, and TDD
test creation including failing test validation and coverage analysis.
"""

import pytest
import asyncio
import tempfile
import shutil
import json
import subprocess
from pathlib import Path
from unittest.mock import Mock, patch, AsyncMock, MagicMock

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from lib.agents.qa_agent import QAAgent
from lib.agents import Task, AgentResult, TDDState
from lib.agent_tool_config import AgentType


class TestQAAgent:
    """Test the QAAgent class."""
    
    @pytest.fixture
    def mock_claude_client(self):
        """Create a mock Claude client."""
        mock_client = Mock()
        mock_client.generate_tests = AsyncMock(return_value="# Generated test suite")
        return mock_client
    
    @pytest.fixture
    def qa_agent(self, mock_claude_client):
        """Create a QAAgent for testing."""
        return QAAgent(claude_code_client=mock_claude_client)
    
    def test_qa_agent_init(self, qa_agent, mock_claude_client):
        """Test QAAgent initialization."""
        assert qa_agent.name == "QAAgent"
        assert qa_agent.claude_client == mock_claude_client
        
        # Check capabilities
        expected_capabilities = [
            "test_creation",
            "test_execution", 
            "coverage_analysis",
            "quality_validation",
            "test_reporting",
            "automated_testing",
            "performance_testing",
            "tdd_test_creation",
            "failing_test_validation",
            "test_red_state_management",
            "test_file_organization",
            "test_preservation"
        ]
        
        for capability in expected_capabilities:
            assert capability in qa_agent.capabilities

    @pytest.mark.asyncio
    async def test_create_tests_dry_run(self, qa_agent):
        """Test test creation in dry run mode."""
        task = Task(
            id="test-1",
            agent_type="QAAgent",
            command="create comprehensive tests",
            context={
                "code": "def calculate_tax(income): return income * 0.2",
                "test_types": ["unit", "integration"]
            }
        )
        
        result = await qa_agent.run(task, dry_run=True)
        
        assert result.success
        assert "[DRY RUN]" in result.output
        assert "unit, integration tests" in result.output
        assert "test_unit.py" in result.artifacts
        assert "test_integration.py" in result.artifacts
        assert "# Generated unit tests" in result.artifacts["test_unit.py"]

    @pytest.mark.asyncio
    async def test_create_tests_with_claude(self, qa_agent, mock_claude_client):
        """Test test creation using Claude client."""
        mock_claude_client.generate_tests.side_effect = [
            "# Unit test code generated by Claude",
            "# Integration test code generated by Claude"
        ]
        
        task = Task(
            id="test-2",
            agent_type="QAAgent",
            command="create test suite",
            context={
                "code": "class UserService:\n    def create_user(self): pass",
                "test_types": ["unit", "integration"]
            }
        )
        
        result = await qa_agent.run(task, dry_run=False)
        
        assert result.success
        assert "Created 2 test suites" in result.output
        assert "test_unit.py" in result.artifacts
        assert "test_integration.py" in result.artifacts
        assert "Claude" in result.artifacts["test_unit.py"]
        assert mock_claude_client.generate_tests.call_count == 2

    @pytest.mark.asyncio
    async def test_create_tests_fallback(self, qa_agent, mock_claude_client):
        """Test test creation fallback when Claude is unavailable."""
        mock_claude_client.generate_tests.side_effect = Exception("Claude unavailable")
        
        task = Task(
            id="test-3",
            agent_type="QAAgent",
            command="create tests",
            context={
                "code": "def process_payment(amount): return True",
                "test_types": ["unit"]
            }
        )
        
        result = await qa_agent.run(task, dry_run=False)
        
        assert result.success
        assert "Created 1 test suites" in result.output
        assert "test_unit.py" in result.artifacts
        assert "class TestUnit" in result.artifacts["test_unit.py"]

    @pytest.mark.asyncio
    async def test_execute_tests_dry_run(self, qa_agent):
        """Test test execution in dry run mode."""
        task = Task(
            id="test-4",
            agent_type="QAAgent",
            command="execute tests",
            context={
                "test_path": "tests/unit/",
                "pattern": "test_*.py"
            }
        )
        
        result = await qa_agent.run(task, dry_run=True)
        
        assert result.success
        assert "[DRY RUN]" in result.output
        assert "test_results.json" in result.artifacts

    @pytest.mark.asyncio
    async def test_execute_tests(self, qa_agent):
        """Test test execution."""
        task = Task(
            id="test-5",
            agent_type="QAAgent",
            command="run test suite",
            context={
                "test_path": "tests/",
                "pattern": "test_*.py"
            }
        )
        
        with patch.object(qa_agent, '_run_test_suite', new_callable=AsyncMock) as mock_run:
            mock_run.return_value = {
                "total": 50,
                "passed": 48,
                "failed": 2,
                "output": "Test execution output",
                "errors": ""
            }
            
            result = await qa_agent.run(task, dry_run=False)
            
            assert result.success
            assert "50 total, 48 passed, 2 failed" in result.output
            assert "test_results.json" in result.artifacts
            mock_run.assert_called_once()

    @pytest.mark.asyncio
    async def test_execute_tests_all_failed(self, qa_agent):
        """Test test execution with failures."""
        task = Task(
            id="test-6",
            agent_type="QAAgent",
            command="execute tests",
            context={"test_path": "tests/"}
        )
        
        with patch.object(qa_agent, '_run_test_suite', new_callable=AsyncMock) as mock_run:
            mock_run.return_value = {
                "total": 10,
                "passed": 5,
                "failed": 5,
                "output": "Some tests failed",
                "errors": "Test errors"
            }
            
            result = await qa_agent.run(task, dry_run=False)
            
            assert not result.success  # Should fail when there are failing tests
            assert "10 total, 5 passed, 5 failed" in result.output

    @pytest.mark.asyncio
    async def test_analyze_coverage_dry_run(self, qa_agent):
        """Test coverage analysis in dry run mode."""
        task = Task(
            id="test-7",
            agent_type="QAAgent",
            command="analyze test coverage",
            context={
                "source_path": "lib/"
            }
        )
        
        result = await qa_agent.run(task, dry_run=True)
        
        assert result.success
        assert "[DRY RUN]" in result.output
        assert "coverage_report.html" in result.artifacts

    @pytest.mark.asyncio
    async def test_analyze_coverage(self, qa_agent):
        """Test coverage analysis."""
        task = Task(
            id="test-8",
            agent_type="QAAgent",
            command="coverage analysis",
            context={
                "source_path": "lib/"
            }
        )
        
        with patch.object(qa_agent, '_run_coverage_analysis', new_callable=AsyncMock) as mock_coverage:
            mock_coverage.return_value = {
                "coverage_percentage": 92.5,
                "total_lines": 2000,
                "covered_lines": 1850,
                "missing_lines": 150
            }
            
            result = await qa_agent.run(task, dry_run=False)
            
            assert result.success
            assert "92.5% coverage" in result.output
            assert "coverage_report.html" in result.artifacts
            mock_coverage.assert_called_once()

    @pytest.mark.asyncio
    async def test_validate_quality(self, qa_agent):
        """Test quality validation."""
        task = Task(
            id="test-9",
            agent_type="QAAgent",
            command="validate code quality",
            context={
                "source_path": "lib/"
            }
        )
        
        with patch.object(qa_agent, '_analyze_code_quality', new_callable=AsyncMock) as mock_quality:
            mock_quality.return_value = {
                "complexity": 2.8,
                "maintainability": 8.7,
                "duplication": 1.2,
                "security_issues": 0,
                "overall_score": 8.9
            }
            
            result = await qa_agent.run(task, dry_run=False)
            
            assert result.success
            assert "8.9/10" in result.output
            assert "quality_report.md" in result.artifacts
            mock_quality.assert_called_once()

    @pytest.mark.asyncio
    async def test_generate_report(self, qa_agent):
        """Test QA report generation."""
        task = Task(
            id="test-10",
            agent_type="QAAgent",
            command="generate qa report",
            context={
                "report_type": "comprehensive"
            }
        )
        
        result = await qa_agent.run(task, dry_run=False)
        
        assert result.success
        assert "comprehensive QA report" in result.output
        assert "qa_report_comprehensive.md" in result.artifacts
        
        # Check report content
        report_content = result.artifacts["qa_report_comprehensive.md"]
        assert "QA Report - Comprehensive" in report_content
        assert "Test Results Summary" in report_content
        assert "Quality Metrics" in report_content

    @pytest.mark.asyncio
    async def test_performance_test(self, qa_agent):
        """Test performance testing."""
        task = Task(
            id="test-11",
            agent_type="QAAgent",
            command="performance test api",
            context={
                "endpoint": "/api/users",
                "load_pattern": "heavy"
            }
        )
        
        with patch.object(qa_agent, '_run_performance_tests', new_callable=AsyncMock) as mock_perf:
            mock_perf.return_value = {
                "avg_response_time": 95,
                "max_response_time": 350,
                "min_response_time": 45,
                "requests_per_second": 1200,
                "error_rate": 0.01
            }
            
            result = await qa_agent.run(task, dry_run=False)
            
            assert result.success
            assert "95ms avg" in result.output
            assert "performance_results.json" in result.artifacts
            mock_perf.assert_called_once()

    @pytest.mark.asyncio
    async def test_write_failing_tests_dry_run(self, qa_agent):
        """Test TDD failing test creation in dry run mode."""
        task = Task(
            id="test-12",
            agent_type="QAAgent",
            command="write_failing_tests",
            context={
                "story_id": "USER-123",
                "specification": "User registration system",
                "acceptance_criteria": ["Valid users can register", "Invalid data rejected"],
                "test_scenarios": ["Happy path", "Edge cases"]
            }
        )
        
        result = await qa_agent.run(task, dry_run=True)
        
        assert result.success
        assert "[DRY RUN]" in result.output
        assert "USER-123" in result.output
        assert "test_red_example.py" in result.artifacts

    @pytest.mark.asyncio
    async def test_write_failing_tests(self, qa_agent):
        """Test TDD failing test creation."""
        task = Task(
            id="test-13",
            agent_type="QAAgent",
            command="tdd_test creation",
            context={
                "story_id": "AUTH-456",
                "specification": "Authentication system with JWT",
                "acceptance_criteria": ["Valid credentials return token", "Invalid credentials rejected"],
                "test_scenarios": ["Login flow", "Token validation"]
            }
        )
        
        with patch.object(qa_agent, '_generate_tdd_test_files', new_callable=AsyncMock) as mock_generate:
            with patch.object(qa_agent, '_organize_tdd_test_files') as mock_organize:
                with patch.object(qa_agent, '_validate_failing_tests', new_callable=AsyncMock) as mock_validate:
                    mock_generate.return_value = {
                        "test_authentication.py": "# Failing test code",
                        "test_jwt_validation.py": "# JWT test code"
                    }
                    
                    mock_organize.return_value = {
                        "tests/tdd/AUTH-456/unit/test_authentication.py": "# Failing test code",
                        "tests/tdd/AUTH-456/integration/test_jwt_validation.py": "# JWT test code"
                    }
                    
                    mock_validate.return_value = {
                        "total_tests": 8,
                        "failing_tests": 8,
                        "passing_tests": 0,
                        "test_errors": 0
                    }
                    
                    result = await qa_agent.run(task, dry_run=False)
                    
                    assert result.success
                    assert "Generated 2 test files" in result.output
                    assert "All tests confirmed to fail initially" in result.output
                    assert "tests/tdd/AUTH-456/unit/test_authentication.py" in result.artifacts
                    assert "test_validation.json" in result.artifacts

    @pytest.mark.asyncio
    async def test_validate_test_red_state(self, qa_agent):
        """Test RED state validation."""
        task = Task(
            id="test-14",
            agent_type="QAAgent",
            command="validate_test_red_state",
            context={
                "story_id": "CART-789",
                "test_files": ["test_cart.py", "test_checkout.py"]
            }
        )
        
        with patch.object(qa_agent, '_validate_single_test_file_red_state', new_callable=AsyncMock) as mock_validate:
            mock_validate.side_effect = [
                {
                    "file": "test_cart.py",
                    "all_tests_failing": True,
                    "failing_for_correct_reasons": True,
                    "status": "Valid RED state"
                },
                {
                    "file": "test_checkout.py",
                    "all_tests_failing": True,
                    "failing_for_correct_reasons": True,
                    "status": "Valid RED state"
                }
            ]
            
            result = await qa_agent.run(task, dry_run=False)
            
            assert result.success
            assert "Files validated: 2" in result.output
            assert "All tests failing: True âœ“" in result.output
            assert "red_state_validation.json" in result.artifacts

    @pytest.mark.asyncio
    async def test_organize_test_files(self, qa_agent):
        """Test test file organization."""
        task = Task(
            id="test-15",
            agent_type="QAAgent",
            command="organize_test_files",
            context={
                "story_id": "PAYMENT-101",
                "test_files": {
                    "test_unit_payment.py": "# Unit test content",
                    "test_integration_gateway.py": "# Integration test content",
                    "test_edge_cases.py": "# Edge case tests"
                }
            }
        )
        
        result = await qa_agent.run(task, dry_run=False)
        
        assert result.success
        assert "tests/tdd/PAYMENT-101" in result.output
        assert "Unit tests: 2" in result.output  # test_unit_payment.py and test_edge_cases.py
        assert "Integration tests: 1" in result.output  # test_integration_gateway.py
        assert "test_organization.json" in result.artifacts

    @pytest.mark.asyncio
    async def test_check_test_coverage(self, qa_agent):
        """Test TDD coverage checking."""
        task = Task(
            id="test-16",
            agent_type="QAAgent",
            command="check_test_coverage",
            context={
                "implementation_files": ["src/user.py", "src/auth.py"],
                "test_files": ["tests/test_user.py", "tests/test_auth.py"],
                "coverage_target": 95
            }
        )
        
        with patch.object(qa_agent, '_run_tdd_coverage_analysis', new_callable=AsyncMock) as mock_coverage:
            mock_coverage.return_value = {
                "overall_coverage": 97.5,
                "critical_path_coverage": 100.0,
                "unit_coverage": 98.0,
                "integration_coverage": 95.0,
                "edge_case_coverage": 92.0,
                "low_coverage_files": []
            }
            
            result = await qa_agent.run(task, dry_run=False)
            
            assert result.success
            assert "97.5% (target: 95%)" in result.output
            assert "100.0% (target: 100%)" in result.output
            assert "Meets TDD standards: True" in result.output
            assert "tdd_coverage_report.json" in result.artifacts

    @pytest.mark.asyncio
    async def test_execute_tdd_phase_test_red(self, qa_agent):
        """Test executing TDD TEST_RED phase."""
        context = {
            "story_id": "FEATURE-202",
            "tdd_specification": "User profile management",
            "acceptance_criteria": ["Users can update profile", "Invalid data rejected"],
            "test_scenarios": ["Profile update flow", "Validation scenarios"],
            "api_contracts": "REST API for profile management"
        }
        
        with patch.object(qa_agent, '_write_failing_tests', new_callable=AsyncMock) as mock_write:
            with patch.object(qa_agent, '_validate_test_red_state', new_callable=AsyncMock) as mock_validate:
                with patch.object(qa_agent, '_organize_test_files', new_callable=AsyncMock) as mock_organize:
                    mock_write.return_value = AgentResult(success=True, output="Tests written", artifacts={})
                    mock_validate.return_value = AgentResult(success=True, output="RED state valid", artifacts={})
                    mock_organize.return_value = AgentResult(success=True, output="Files organized", artifacts={})
                    
                    result = await qa_agent.execute_tdd_phase(TDDState.TEST_RED, context)
                    
                    assert result.success
                    assert "TDD TEST_RED Phase Complete" in result.output
                    assert "Ready for CODE_GREEN Phase" in result.output

    @pytest.mark.asyncio
    async def test_execute_tdd_phase_invalid(self, qa_agent):
        """Test executing invalid TDD phase."""
        context = {"story_id": "test"}
        
        result = await qa_agent.execute_tdd_phase(TDDState.DESIGN, context)
        
        assert not result.success
        assert "can only execute TEST_RED phase" in result.error

    @pytest.mark.asyncio
    async def test_general_qa_task(self, qa_agent):
        """Test handling of general QA tasks."""
        task = Task(
            id="test-17",
            agent_type="QAAgent",
            command="custom qa operation",
            context={}
        )
        
        result = await qa_agent.run(task, dry_run=False)
        
        assert result.success
        assert "QAAgent executed: custom qa operation" in result.output

    @pytest.mark.asyncio
    async def test_error_handling(self, qa_agent):
        """Test error handling during task execution."""
        task = Task(
            id="test-18",
            agent_type="QAAgent",
            command="create tests",
            context={"code": "test"}
        )
        
        with patch.object(qa_agent, '_create_tests', side_effect=Exception("Test error")):
            result = await qa_agent.run(task, dry_run=False)
            
            assert not result.success
            assert "Test error" in result.error
            assert result.execution_time > 0

    # Test helper methods
    
    @pytest.mark.asyncio
    async def test_run_test_suite_success(self, qa_agent):
        """Test successful test suite execution."""
        with patch('subprocess.run') as mock_subprocess:
            mock_subprocess.return_value = Mock(
                stdout="test_file.py::test_function PASSED\ntest_file.py::test_function2 PASSED\n",
                stderr="",
                returncode=0
            )
            
            result = await qa_agent._run_test_suite("tests/", "test_*.py")
            
            assert isinstance(result, dict)
            assert result["total"] == 2
            assert result["passed"] == 2
            assert result["failed"] == 0

    @pytest.mark.asyncio
    async def test_run_test_suite_with_failures(self, qa_agent):
        """Test test suite execution with failures."""
        with patch('subprocess.run') as mock_subprocess:
            mock_subprocess.return_value = Mock(
                stdout="test_file.py::test_pass PASSED\ntest_file.py::test_fail FAILED\n",
                stderr="",
                returncode=1
            )
            
            result = await qa_agent._run_test_suite("tests/", "test_*.py")
            
            assert result["total"] == 2
            assert result["passed"] == 1
            assert result["failed"] == 1

    @pytest.mark.asyncio
    async def test_run_test_suite_exception(self, qa_agent):
        """Test test suite execution with exception."""
        with patch('subprocess.run', side_effect=Exception("Process error")):
            result = await qa_agent._run_test_suite("tests/", "test_*.py")
            
            assert result["total"] == 0
            assert result["passed"] == 0
            assert result["failed"] == 1
            assert "Process error" in result["errors"]

    @pytest.mark.asyncio
    async def test_run_coverage_analysis(self, qa_agent):
        """Test coverage analysis execution."""
        with patch('subprocess.run') as mock_subprocess:
            mock_subprocess.return_value = Mock(
                stdout="",
                stderr="",
                returncode=0
            )
            
            result = await qa_agent._run_coverage_analysis("lib/")
            
            assert isinstance(result, dict)
            assert "coverage_percentage" in result
            assert result["coverage_percentage"] == 85  # Placeholder value

    @pytest.mark.asyncio
    async def test_run_coverage_analysis_error(self, qa_agent):
        """Test coverage analysis with error."""
        with patch('subprocess.run', side_effect=Exception("Coverage error")):
            result = await qa_agent._run_coverage_analysis("lib/")
            
            assert result["coverage_percentage"] == 0
            assert "Coverage error" in result["error"]

    @pytest.mark.asyncio
    async def test_analyze_code_quality(self, qa_agent):
        """Test code quality analysis."""
        result = await qa_agent._analyze_code_quality("lib/")
        
        assert isinstance(result, dict)
        assert "complexity" in result
        assert "maintainability" in result
        assert "duplication" in result
        assert "security_issues" in result
        assert "overall_score" in result

    @pytest.mark.asyncio
    async def test_run_performance_tests(self, qa_agent):
        """Test performance test execution."""
        result = await qa_agent._run_performance_tests("/api/test", "standard")
        
        assert isinstance(result, dict)
        assert "avg_response_time" in result
        assert "max_response_time" in result
        assert "min_response_time" in result
        assert "requests_per_second" in result
        assert "error_rate" in result

    def test_mock_test_results(self, qa_agent):
        """Test mock test results generation."""
        result = qa_agent._mock_test_results()
        
        assert isinstance(result, dict)
        assert "total" in result
        assert "passed" in result
        assert "failed" in result
        assert result["total"] == 25
        assert result["passed"] == 23
        assert result["failed"] == 2

    def test_generate_test_suite(self, qa_agent):
        """Test test suite generation."""
        code = "def calculate_sum(a, b): return a + b"
        test_suite = qa_agent._generate_test_suite(code, "unit")
        
        assert isinstance(test_suite, str)
        assert "class TestUnit" in test_suite
        assert "def test_happy_path" in test_suite
        assert "def test_error_handling" in test_suite
        assert "def test_edge_cases" in test_suite
        assert "import unittest" in test_suite
        assert "import pytest" in test_suite

    def test_generate_test_suite_integration(self, qa_agent):
        """Test integration test suite generation."""
        code = "class DatabaseService: pass"
        test_suite = qa_agent._generate_test_suite(code, "integration")
        
        assert "class TestIntegration" in test_suite
        assert "integration" in test_suite.lower()

    def test_generate_coverage_report(self, qa_agent):
        """Test coverage report generation."""
        coverage_data = {
            "coverage_percentage": 87.5,
            "total_lines": 2000,
            "covered_lines": 1750,
            "missing_lines": 250
        }
        
        report = qa_agent._generate_coverage_report(coverage_data)
        
        assert isinstance(report, str)
        assert "Test Coverage Report" in report
        assert "87.5%" in report
        assert "2000" in report
        assert "1750" in report
        assert "250" in report
        assert "<html>" in report
        assert "coverage-high" in report

    def test_generate_quality_report(self, qa_agent):
        """Test quality report generation."""
        quality_metrics = {
            "complexity": 4.2,
            "maintainability": 7.8,
            "duplication": 3.1,
            "security_issues": 1,
            "overall_score": 7.5
        }
        
        report = qa_agent._generate_quality_report(quality_metrics)
        
        assert isinstance(report, str)
        assert "Code Quality Report" in report
        assert "4.2" in report
        assert "7.8" in report
        assert "3.1" in report
        assert "1" in report
        assert "7.5/10" in report
        assert "Recommendations" in report

    def test_create_qa_report(self, qa_agent):
        """Test QA report creation."""
        report = qa_agent._create_qa_report("summary")
        
        assert isinstance(report, str)
        assert "QA Report - Summary" in report
        assert "Test Results Summary" in report
        assert "Quality Metrics" in report
        assert "Critical Issues" in report
        assert "Recommendations" in report
        assert "Risk Assessment" in report

    def test_organize_tdd_test_files(self, qa_agent):
        """Test TDD test file organization."""
        test_files = {
            "test_user_unit.py": "# Unit test",
            "test_user_integration.py": "# Integration test",
            "test_general.py": "# General test"
        }
        
        organized = qa_agent._organize_tdd_test_files(test_files, "USER-123")
        
        assert isinstance(organized, dict)
        assert any("unit" in path for path in organized.keys())
        assert any("integration" in path for path in organized.keys())
        assert "tests/tdd/USER-123" in list(organized.keys())[0]

    @pytest.mark.asyncio
    async def test_generate_tdd_test_files(self, qa_agent):
        """Test TDD test file generation."""
        specification = "User authentication system"
        acceptance_criteria = ["Valid users can login", "Invalid credentials rejected"]
        test_scenarios = ["Login flow", "Error handling"]
        story_id = "AUTH-789"
        
        # Mock the method since it's complex
        with patch.object(qa_agent, '_generate_tdd_test_files', new_callable=AsyncMock) as mock_generate:
            mock_generate.return_value = {
                "test_authentication.py": "# Generated auth tests",
                "test_login_flow.py": "# Generated login tests"
            }
            
            result = await qa_agent._generate_tdd_test_files(
                specification, acceptance_criteria, test_scenarios, story_id
            )
            
            assert isinstance(result, dict)
            assert len(result) == 2
            mock_generate.assert_called_once()

    @pytest.mark.asyncio
    async def test_validate_failing_tests(self, qa_agent):
        """Test failing test validation."""
        test_files = {
            "test_example.py": "# Test content"
        }
        
        # Mock the method since it involves test execution
        with patch.object(qa_agent, '_validate_failing_tests', new_callable=AsyncMock) as mock_validate:
            mock_validate.return_value = {
                "total_tests": 5,
                "failing_tests": 5,
                "passing_tests": 0,
                "test_errors": 0
            }
            
            result = await qa_agent._validate_failing_tests(test_files)
            
            assert isinstance(result, dict)
            assert result["total_tests"] == 5
            assert result["failing_tests"] == 5
            mock_validate.assert_called_once()

    @pytest.mark.asyncio
    async def test_validate_single_test_file_red_state(self, qa_agent):
        """Test single test file RED state validation."""
        # Mock the method since it involves test execution
        with patch.object(qa_agent, '_validate_single_test_file_red_state', new_callable=AsyncMock) as mock_validate:
            mock_validate.return_value = {
                "file": "test_example.py",
                "all_tests_failing": True,
                "failing_for_correct_reasons": True,
                "status": "Valid RED state"
            }
            
            result = await qa_agent._validate_single_test_file_red_state("test_example.py")
            
            assert isinstance(result, dict)
            assert result["all_tests_failing"] is True
            assert result["failing_for_correct_reasons"] is True
            mock_validate.assert_called_once()

    @pytest.mark.asyncio
    async def test_run_tdd_coverage_analysis(self, qa_agent):
        """Test TDD-specific coverage analysis."""
        implementation_files = ["src/user.py", "src/auth.py"]
        test_files = ["tests/test_user.py", "tests/test_auth.py"]
        
        # Mock the method since it involves coverage tools
        with patch.object(qa_agent, '_run_tdd_coverage_analysis', new_callable=AsyncMock) as mock_coverage:
            mock_coverage.return_value = {
                "overall_coverage": 95.0,
                "critical_path_coverage": 100.0,
                "unit_coverage": 97.0,
                "integration_coverage": 90.0,
                "edge_case_coverage": 85.0,
                "low_coverage_files": []
            }
            
            result = await qa_agent._run_tdd_coverage_analysis(implementation_files, test_files)
            
            assert isinstance(result, dict)
            assert result["overall_coverage"] >= 90
            assert result["critical_path_coverage"] == 100
            mock_coverage.assert_called_once()